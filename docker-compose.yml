version: '3.8'

services:
  # 1. Web Application (FastAPI)
  book-web:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: book-web-app
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - QDRANT_URL=${QDRANT_URL}
      - N8N_URL=${N8N_URL}
      - OLLAMA_URL=${OLLAMA_URL}
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      n8n:
        condition: service_started
    restart: unless-stopped
    networks:
      - rag-network

  # 2. Database (PostgreSQL)
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}']
      interval: 5s
      timeout: 5s
      retries: 10
    networks:
      - rag-network

  # 3. Vector Database (Qdrant)
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - rag-network

  # 4. Workflow Automation (N8N)
  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    ports:
      - "5678:5678"
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_USER=${POSTGRES_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_HOST=n8n
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - rag-network

  # 5. LLM Inference (Ollama) - GPU Support Check
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - rag-network

  # 6. Model Puller (Auto-pull models on start)
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - "sleep 5; OLLAMA_HOST=ollama:11434 ollama pull llama3.2; OLLAMA_HOST=ollama:11434 ollama pull nomic-embed-text"
    depends_on:
      - ollama
    networks:
      - rag-network

volumes:
  postgres_data:
  qdrant_data:
  n8n_data:
  ollama_data:

networks:
  rag-network:
    driver: bridge
